\documentclass[12pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format

\usepackage[dvipsnames]{xcolor}


\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}

\usepackage[left=0.75in, right=0.75in, top=1in, bottom=1in]{geometry}   
\linespread{1.5}
\usepackage{hyperref}             		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\usepackage{algorithm2e}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}






\title{Artificial Intelligence and Strategic Opportunities for the Miller Center}
\author{Miles Efron}
\date{
 Miller Center of Public Affairs \\
 University of Virginia \\
 mefron@virginia.edu \\
 \bigskip
 \today
}							% Activate to display a given date or no date

\begin{document}
\maketitle

% \abstract{Since the release of ChatGPT in 2022, Artificial Intelligence (AI) technologies have seized the public’s attention.  For institutions such as the Miller Center, these technologies offer opportunities to improve institutional impact and to modernize workflows. But how to capitalize on these opportunities, and how to avoid their  pitfalls is not obvious.  To make sense of AI’s promises and risks, this white paper undertakes a wholesale consideration of how Miller Center leadership could marshal AI technology  to further the center’s mission and work.  The white paper consists of four main parts.  We begin with an overview of important definitions and vocabulary.  Second, we give a detailed census of the data available for AI work at the Miller Center, with an eye towards understanding how our data holdings could be brought to bear on future technological initiatives.  Third, we outline the institutional context in which any Miller Center AI work would take place, focusing on AI initiatives  currently under way across grounds at UVA.  Lastly, the paper concludes with a set of recommendations for future work—a listing of variously ambitious data-intensive projects that the Miller Center could undertake to increase its overall institutional impact and success.}

\pagebreak
\tableofcontents
\pagebreak


\section{Executive Summary}\label{section.summary}
Since the release of ChatGPT in 2022, Artificial Intelligence (AI) technologies have seized the public’s attention.  For institutions such as the Miller Center, these technologies offer opportunities to improve institutional impact and to modernize workflows. But how to capitalize on these opportunities, and how to avoid their  pitfalls is not obvious.  To make sense of AI’s promises and risks, this white paper undertakes a wholesale consideration of how Miller Center leadership could marshal AI technology  to further the center’s mission and work.  

The paper consists of three main parts.  First we develop a  vocabulary for discussing AI, outlining key terms and functionalities that form our shared vocabulary for talking about AI.  Data is the coin of the realm in AI, driving what is possible and how we  implement solutions.  Recognizing data's practical importance, Section \ref{section.data} enumerates the key data sets available for AI work at the Miller Center.  To give a sense of how the Miller Center fits into the larger AI landscape at UVA, Section \ref{section.grounds} outlines UVA's most vital current and recently completed AI initiatives.  The remainder of the white paper enumerates seven projects that the Miller Center could undertake as an initial foray into using AI for its work.  By design, these projects span a wide array of risk/reward profiles, giving leadership a sense of what is possible and a range of commitments and ambitions to consider.

The project proposals in Section \ref{section.applications} are detailed and dense.  To give a high-level sense of our recommendations, Table \ref{table.summary} summarizes the projects we outline later in this paper.  The table gives a sense of the range of options available to the Miller Center.  


\begin{table}[htp]
\caption{A summary of AI projects proposed in this white paper.  Projects are ranked in increasing order of difficulty and cost.}
\begin{center}
\begin{tabular}{ p{4in} p{0.75in} l l }
\toprule
Project													&	Difficulty 			&	Cost		&	Section	\\
\midrule
					\multicolumn{4}{c}{Easy Projects}														\\
\midrule
 Improving mc.org site search								&	\colorbox{green}{Low}	& \colorbox{green}{Low}		&	\ref{section.applications.easy.sitesearch}	\\
 \midrule
Informally integrating AI tools into Miller Center work			&	\colorbox{green}{Low}	& \colorbox{Goldenrod}{Medium}		&	\ref{section.application.easy.daily-work}	\\
\midrule
 Expanding our data portal								&	\colorbox{green}{Low}	& \colorbox{Goldenrod}{Medium}		&	\ref{section.applications.easy.data-portal}	\\
 \midrule

%Open database of presidential deepfakes						&	\colorbox{Goldenrod}{Low}	& \colorbox{Goldenrod}{Medium}		&	\ref{section.applications.easy.sitesearch}	\\
 %\midrule
 					\multicolumn{4}{c}{More Ambitious Projects}														\\
\midrule
LLMs to extract OHP interview metadata						&	\colorbox{Goldenrod}{Medium}	& \colorbox{green}{Low}		&	\ref{section.applications.hard.mcllm}	\\
\midrule
Toolkit to support PRP scholarship							&	\colorbox{red}{High}	& \colorbox{Goldenrod}{medium}		&	\ref{section.applications.hard.prp}	\\
\midrule
A Miller Center language model (internal deployment)			&	\colorbox{red}{High}	& \colorbox{green}{Low}		&	\ref{section.applications.hard.mcllm}	\\	
\midrule
A Miller Center language model (public deployment)				&	\colorbox{red}{High}	& \colorbox{red}{High}		&	\ref{section.applications.hard.mcllm}	\\		
\bottomrule
\end{tabular}
\end{center}
\label{table.summary}
\end{table}%

\pagebreak 

\section{Introduction}\label{section.introduction}
In 2022, Artificial Intelligence (AI) decisively stepped out of the shadows of theoretical computer science and into the limelight of practical, impactful technology. This transition was fueled by notable advancements in AI technology and an increasing ease of access to AI tools.  With AI at the front of public attention and imagination, there is a growing sense of possibility but also anxiety around AI.  AI tools support truly novel workflows, allowing people to survey and capitalize on data at previously unseen scales.  But the power of these tools and their novelty makes them hard to understand.  Exactly how to use these new tools is an urgent question without obvious answers.

Nowhere are these developments more keenly felt than in universities.  Academia, with its traditions of scholarly exploration, argumentation from data, and the free exchange of ideas, is uniquely situated to make use of AI.  However, the same challenges obtain in universities as anywhere else, perhaps more so.  There is a sense that AI presents a singular opportunity for academia.  But how to meet this opportunity is a profound challenge.  In light of this challenge, university faculty, staff, and administrators have begun to organize their thinking and efforts, forming committees, research programs, and initiatives to study how to integrate AI into their work.  2024, it seems, will be a watershed year for the fate of AI in academia.

As a scholarly unit with a huge portfolio of machine-readable data and a history of technical innovation, the Miller Center of Public Affairs at UVA is poised to find uses for AI that both increase the impact of Miller Center work and that catch the public attention.   Miller Center leadership and staff have already recognized that AI presents us with a chance to increase the impact of our work.  In terms of outward-facing impact, there is a sense that AI will help us share our results more broadly than we could before.  In terms of internal workflows, Miller Center scholars and staff already use AI on a daily basis (as of 2024 everyday software such as Google, MS Word, and Adobe Photoshop all use AI), and it seems unlikely that the footprint of AI on our workflows to shrink.

To help synthesize these issues and to recommend a strategic posture with respect to AI, Miller Center leadership commissioned this white paper.  The goal of the paper is twofold:

\begin{enumerate}
\item To give readers at the Miller Center a shared context (vocabulary, historical framework) for understanding what factors are in play when we discuss AI and academia in 2024.
\item To propose a slate of possible projects that would bring AI technologies to bear on Miller Center work in tangible, impactful ways.
\end{enumerate}

This paper starts with background and crucial definitions for understanding AI as it pertains to Miller Center priorities and work.  A high-level census of Miller Center data follows.  The aim of the data census is to give a basis for understanding what kinds of model training and evaluation would be feasible for the Miller Center.  The final section of the paper enumerates several AI projects that the Miller Center could undertake.  Our goal in this enumeration is to offer projects of varying ambition and varying risk, from simple ``low-hanging fruit'' to cutting-edge deployments that would entail research activity in their own right.  Overall, our hope is to give a sense of what is possible for the Miller Center, in efforts to spark a conversation about what is desireable.


\section{Definitions and Descriptions}\label{section.definitions}
To ground our discussion of AI and its application to the Miller Center's work, this section offers definitions and basic vocabulary.  Each of the terms listed in this section is complex and multifaceted.  Our aim in defining these terms is to give all readers a shared sense of how each term fits into this paper.  That is, our goal is not so much to give conclusive definitions of each term, but instead to explain how each term will fit into the discussion presented specifically in this white paper.


\subsection{Artificial Intelligence}\label{section.definitions.artificial-intelligence}
Though the term Artificial Intelligence (AI) was coined in the 1940’s, it took on new meaning in recent years.  AI used to refer to systems that relied on symbolic logic to mimic human inference.  Now, people talking about AI are usually referring to systems that rely on massive data and statistical models to mimic human creativity.  What we currently call artificial intelligence is best understood as the confluence of three related developments:

\begin{itemize}
\item Cheap, plentiful computation has become pervasive.  This makes once-impossible computational problems tractable. Cloud computing and advanced networking make it simple to build heroically powerful supercomputers.
\item Electronic data is abundant and inexpensive.  The internet has reached a size and a level of professionalism in its technical underpinnings that make it feasible to acquire, store, and recall data on a scale that has never been possible before.
\item Deep learning expanded the power of machine learning. Research in machine learning has matured in a way that allows programmers to build predictive models that that solve human problems with previously unseen accuracy and flexibility.   
\end{itemize}

Today, discussions of AI usually concern this trifecta, an intersection of historical events that together allow computers to do things that we recognize as wholly novel, tasks that were formerly limited to human agency.

For the remainder of this white paper, we will use the term \emph{artificial intelligence} to refer to \emph{machine learning systems that use massive-scale data to approximate human skill on creative tasks such as writing, editing, and data analysis.}.   This definition is also sometimes called \emph{generative AI} because these AI systems are capable of generating novel artifacts such as texts, images, and audio recordings.


\subsection{Deep Learning}\label{section.definitions.deep-learning}
The term \emph{machine learning} refers to the practice of training computers to recognize patterns from data.  Machine learning is a well-established academic field, straddling computer science and statistics.  In the early 2000’s, the state of the art in machine learning suddenly and fundamentally changed, when researchers sparked a novel innovation—\emph{deep learning.}  Deep learning has become the mainstay of modern machine learning, and it forms the backbone of the current generation of AI systems.

The hallmark of deep learning is the size and structure of its models.  Earlier machine learning algorithms used their training data to estimate a relatively small number of model parameters.  For instance, a classical email spam filter guessed the status (spam or non-spam) of an incoming message by observing the word tokens in the message.  The filter would look up how strongly each word is associated with spam messages versus non-spam messages.  Each word's weight was a parameter of the model. By summing these weights over the terms in a message, the system could reasonably predict the likelihood that a person would judge the message to be spam.  In this scenario, the model consists of two weights (spamminess and non-spamminess) for each word in the English language (assuming the user is an English speaker)--on the order of tens of thousands of parameters.  Different algorithms varied in how they estimated these weights.  But the basic structure of the model (a pair of weights per word type) and the weighted sum operation for inference  were shared by most algorithms.

Deep learning fundamentally changed this state of affairs.  Instead of tens of thousands of parameters, deep learning models have millions or billions, and in some cases trillions, of parameters.  As in older models, deep learning has weights analogous to those described above; in a deep learning email filter, we would indeed see weights for each word in the English language.  But in addition to these, deep learning contains armies of secondary parameters.  Collectively, these so-called “hidden layers” of the model capture relationships between observed variables such as words.  For instance, this architecture allows, a deep learning model to learn that the phrase \emph{machine learning} is a single concept, an idea greater than the sum of its lexical parts.  This novel architecture qualitatively changed the sophistication and expressiveness of machine learning models.



\subsection{Large Language Models}\label{section.definitions.llms}
Much of the hype around AI concerns so-called large language models (LLMs).  As their name suggests, LLMs are deep learning models that attempt to mimic human linguistic behavior.  OpenAI’s GPT-4, Google’s Gemini, and Facebook’s Llama are all LLMs.  The accuracy and flexibility of LLMs is perhaps the most striking development in the landscape of current AI research and development.  Due to the Miller Center’s focus on scholarly communication and political insight, LLMs will also be of special interest as we consider how the center can make use of AI in the future.

A particularly important distinction between older models and LLMs is that LLMs are trained to perform well on a variety of linguistic tasks, while older models were narrowly scoped.  In our example above we mentioned an email spam filter.  This is a classic example of older-generation machine learning.  Such a filter is intended to be used in a narrow setting, for a single task--all it does is guess if incoming email messages are spam.  On the other hand, LLMs like GPT-4 or Llama model general linguistic fluency.  They can perform well on a variety of loosely structured tasks such as summarization, translation, or even writing from scratch.  In fact, an interesting result of modern AI is that researchers often do not know what tasks their models will do well on, and an important research goal is finding and demonstrating novel uses of LLMs.  This is likely to be a part of the work we at the Miller Center undertake as we explore AI suitability for our needs.

This newfound model fluency and flexibility is partly due to the novel model structure afforded by deep learning.  Without deep learning’s vast architectural complexity, LLMs would be unable to encode human knowledge so accurately.  But the other crucial factor in this development is an abundance of training data.  Older models, with more parsimonious structures, could be trained with small sets of data.  LLMs, on the other hand, require terabytes, even petabytes of data before they can perform well.  Figure \ref{figure.llm_size} shows the growth of LLM parameter spaces between 2018 and 2023\footnote{Information about the evolution of model size is available in several sources in this paper’s Works Cited section, such as \cite{shazeer:2017, wei:2022}.}.  Early models such as GPT-1 learned on the order of 100 million parameters, while the current state of the art (e.g. GPT-4) boasts over 100 trillion parameters.  This increase has improved the accuracy and fluency of LLMs.  But increasing model size demands a corresponding increase in training data.  Training data on the order of 100 gigabytes was common in 2015.  But current models demand multiple terabytes to reach a stable, fully trained state.  For instance, OpenAI has published results showing that their GPT-3 model required 45 terabytes (and \$100M) for proper training \cite{brown:2020}. Because these models require so much data to reach their potential, they are expensive to build, and an important fact of modern AI work is that instead of training new models for each deployment, it is incumbent on AI professionals to find creative ways to re-use pre-trained models.


\begin{figure}[htbp]
\begin{center}
\includegraphics{./figures/llm_params.png}
\caption{Large language models and the sizes of their parameter space, 2018-2023.}
\label{figure.llm_size}
\end{center}
\end{figure}


\subsection{Foundation Models and Model Fine-Tuning}\label{section.definitions.foundation-models}

Because current LLMs are so expensive and time-consuming to train, people often call them \emph{foundation models.}  A foundation model is a deep learning model (often an LLM, though others operate on image or audio tasks) that is sufficiently performant and sufficiently flexible to adapt to a wide range of tasks.  

Once we choose a foundation model upon which to base our work, the second, and often the most challenging task, lies in getting our foundation model to treat our problems and data with sufficient sensitivity.  For instance, GPT-4 (with over 100 trillion parameters) is very powerful.  But its exposure to, say, information about presidential history, is only fleeting.  To use GPT-4 to perform tasks on Miller Center data, it is likely that we would need to supplement its knowledge in efforts to focus its output on topics relevant to our audience.

There are numerous ways to train a foundation model to work well in a specific setting.  The most direct approach is called \emph{model fine-tuning.}  This task involves deploying a local copy of the model and showing it a corpus of training data indicative of the task at hand.  Researchers have shown that even a modest amount of training data (on the order of 50-100 training examples) can quickly focus a foundation model on almost any task \cite{liu:2022}.

\subsection{Retrieval-Augmented Generation}\label{section.definitions.rag}
Unfortunately even fine-tuned models make mistakes.  An important point to understand about LLMs is that they contain no database of facts, no knowledge about the world aside from their understanding of language.  When LLMs create text, they form statements that are statistically likely, but no additional fact checking is part of their standard operating procedure.  Fortunately, an LLM's notion of linguistic appropriateness is so deep that it is often enough to keep them correct and factual.  But occasionally, LLMs will \emph{hallucinate}, creating text that seems plausible but is factually wrong.  In settings where accuracy (as opposed, say, to creativity) is crucial, this tendency requires us to add guardrails when deploying LLMs.

Perhaps the most broadly implemented guardrail architecture is known as \emph{retrieval-augmented generation} (RAG).  A RAG system consists of an LLM and a more traditional search engine.  When a user submits a prompt to a RAG system, the interaction runs like this:
\begin{enumerate}
\item The prompt is presented to the LLM, which translates the user input into one or more queries for the search engine.
\item The search engine scours its database, retrieving documents that appear to contain an answer to the user's queries.
\item The LLM reads and summarizes the retrieved documents, along with the user's original prompt.  Based on these texts, the LLM creates a summary that answers the original question.
\end{enumerate}
The chief point to understand about RAG is that the LLM is only in charge of the natural language portion of the task--translating prompts and interpreting documents.  The factual matters are handled by the search engine.  So long as the search engine has indexed accurate documents, the end user should be able to interact with the system via natural language while also receiving correct information.  By tying the LLM's output to factual data from a document corpus, a RAG system can usually avoid the hallucination problem.




\subsection{Prompt Engineering}\label{section.vocab.prompt-engineering}
In the context of AI and LLMs, a \emph{prompt} is the information an end user gives to a model to articulate his or her information need.  Research has shown that the quality of a prompt has a huge effect on the quality of a model's output.  In fact, improving prompt quality has been shown to be more important for output quality than model size or fine-tuning. \cite{white:2023}.  

The practice of crafting a high-quality AI prompt is known as \emph{prompt engineering}.  Prompt engineering is so fundamental that alongside its general ChatGPT documentation, OpenAI maintains documentation solely dedicated to prompt engineering\footnote{\href{https://platform.openai.com/docs/guides/prompt-engineering}{https://platform.openai.com/docs/guides/prompt-engineering}}.  Likewise, many courses are available to help people improve their prompt engineering skills\footnote{e.g. \href{https://www.coursera.org/learn/prompt-engineering}{Coursera Prompt Engineering for ChatGPT}, \href{https://www.udemy.com/course/prompt-engineering-for-ai}{Udemy prompt engineering course}, \href{https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/}{Deeplearning.ai course on prompt engineering for developers}. }.



 


\subsection{Concluding Thoughts about Vocabulary}\label{section.definitions.closing}
This section has introduced numerous terms of art.  Of chief importance, however, is the fact that collectively, these terms have a great deal in common.  They refer to a recent, ongoing revolution in how we perform computation over data.  This revolution has three hallmarks:
\begin{itemize}
\item It is due to the confluence of cheap computation and plentiful data storage.
\item It benefits from startling advances in the statistical sophistication of freely available computing tools.
\item It is structured by best practices in using computation and data to support the work of scholars and decision-makers.
\end{itemize}
In the remainder of this white paper, we will  refer to the entirety of this zeitgeist as \emph{AI}.  In other words, we use \emph{AI} to refer to deep learning models deployed in configurations that allow people to capitalize on data at a massive scale.


\section{Census of Miller Center Data}\label{section.data}
The coin of the realm in modern AI applications is machine-readable data.  When considering how AI can be useful to an enterprise, the organization’s data drives the discussion in at least three ways:
\begin{enumerate}
\item The topics, genres and themes expressed in the data (along with the enterprise's mission and driving interests) will dictate the questions and prompts that will be posed against AI systems built by the organization.
\item Rights management and other questions of data accessibility and reusability will structure how the data can be used during AI training and deployment.
\item The magnitude, structure and formats of the data will bear on what kinds of AI tasks the unit can complete (e.g. model training and fine-tuning).
\end{enumerate}
Because data are central to the details of deploying AI, this section offers a census of the Miller Center’s machine-readable data holdings.  Our goal here is to describe the “crown jewels” of Miller Center data, not to enumerate every byte of data we own.  Collectively, these descriptions will help us structure subsequent thinking about what types of inquiry and engineering would be useful and practical. for the Miller Center


\subsection{Presidential Recordings}\label{section.data.prp}
The Miller Center is unique in its collection of original and secondary information concerning audio recordings of U. S. presidents.  Miller Center scholars have published these recordings, as well as archival finding aids describing them, on \href{https://millercenter.org}{millercenter.org}.  Additionally, scholars in the Presidential Recordings Program (PRP), continually create scholarly transcripts (in TEI XML format) of the tapes, which they publish in partnership with UVA Press.   The original recordings and archival finding aids are in the public domain.  Due to licensing restrictions, the PRP transcriptions of the tapes are copyright protected.   

Table \ref{table.data.prp} outlines the size of the presidential recording data.  The table is based on the holdings in \href{https://millercenter.org}{mc.org}.  This is admittedly not the full slate of presidential recording data, but the number is close to the total available and gives an accurate sense of the corpus’ dimensions.  The ``Total Hours" column lists the total running time of all available recordings.  The ``Number of Recordings in mc.org” column shows the number of discrete conversations currently (as of this writing) housed on our website.  This figure is equal to or less than the full conversation count, since in the case of a few presidents, we do not yet have all conversations online.  

\begin{table}[htp]
\caption{Miller Center data holdings: presidential pecordings.}
\begin{center}
\begin{tabular}{ p{2in} l l }
\toprule
President				&	Total Hours		&	\#  in $\mathtt{mc.org}$	\\
\midrule
 Franklin Roosevelt 		& 	8 				& 	23 \\ 
 \midrule 
 Harry Truman 			& 	10 				& 	20 \\  
 \midrule
 Dwight Eisenhower	 	& 	15 				& 	36   \\
 \midrule
 John F. Kennedy	 	& 	260 				& 	386   \\
 \hline
 Lyndon Johnson	 	& 	800 				& 	9,497   \\
 \midrule
 Richard Nixon		 	& 	3,700			& 	23,136   \\
\bottomrule
Total					&	4,793			&	33,098	\\
\bottomrule
\end{tabular}
\end{center}
\label{table.data.prp}
\end{table}%

In addition to the public domain data described in Table 2, PRP scholars at the Miller Center have published a subset of conversations, along with carefully edited transcripts, via the UVA Press’s Presidential Recordings Desktop Edition (PRDE).  Table 3 outlines the PRDE holdings, as of this writing.  The total running time of the 4,547 recordings in PRDE is approximately 516 hours.  Currently, access to the PRDE transcripts is limited to subscribers only.  An important area of collaboration in the future will be discussing with UVA Press options for widening the availability of these data.  In initial conversations during this paper's writing, contacts at UVA Press indicated enthusiasm about collaborating with the Miller Center (especially around data sharing arrangements).  A likely follow-on to this work will involve detailed discussions about this possibility.   

\begin{table}[htp]
\caption{Number of recordings in PRDE.}
\begin{center}
\begin{tabular}{ p{1.75in} p{2in}}
\toprule
President				&	Number of Conversations		\\
\midrule
 John F. Kennedy	 	& 	340 				\\
\midrule
 Lyndon Johnson	 	& 	3,972 				\\
\midrule
 Richard Nixon		 	& 	235			 \\
\bottomrule
Total					&	4,547		\\
\bottomrule
\end{tabular}
\end{center}
\label{table.data.prde}
\end{table}%

In terms of data formats, the recordings are stored in mp3 at a minimum (many are stored in multiple formats).  Finding aids are in modern PDF format, which means that it is easy to extract the raw text from them.  The conversations listed in Table \ref{table.data.prde} have accompanying TEI XML-encoded transcripts.  

One closing note is in order about our PRP data.  Because the transcripts of these tapes are likely to be governed by copyright, a possibly useful approach would involve obtaining transcripts of the tapes via AI.  Current-generation audio-to-text AI is capable of transcribing at least the less noisy telephone conversations within the PRP corpus.  The transcriptions would not be suitable for scholarly purposes.  For instance, they are likely to contain some errors and they would not ascribe statements to their human speakers.  However, for tasks like metadata generation (e.g. for indexing), these rough transcriptions may be useful.   

\subsection{Presidential Oral Histories}\label{section.data.pohp}
As of December 2023, the Presidential Oral History Project (POHP) at the Miller Center has conducted over 700 interviews, spanning eight programs (Carter, Reagan, Bush 41, Clinton, Bush 43, Obama, Hillary Clinton, and Edward M. Kennedy).  Each of these interviews generates numerous documents, including the raw audio recording of the interview, interviewee briefing books, and the final transcript.  Likewise, each of these documents is a rich, long-form piece of scholarship or primary-source history.  Some of these documents have been deeded and cleared for publication, while others remain confidential.  

The sensitive nature of undeeded interviews, and indeed the personal character of \emph{all} POHP data make it unique among the Miller Center’s holdings.  For the purposes of this data census, an exhaustive accounting of these data is unnecessary.  Instead, for our purposes it is sufficient to put into the foreground the following statistics:
\begin{itemize}
\item Total POHP programs:  8
\item Total interviews conducted as of this writing: $>$700
\item Number  of interviews published on millercenter.org:  504.
\item Number  of briefing books published on millercenter.org: 348.
\item Number of PDF pages of interviews published on millercenter.org: 25,634
\end{itemize}
We focus here on the subset of OHP data that has been published on the web because these data have been cleared and would form the basis for inclusion in any AI model.  The remaining OHP interviews (i.e. those not shown in this section's enumeration) may find their way into an AI system at a later time.  But until they are deeded, these documents cannot be added to any proposed AI system.

 


\subsection{Miller Center Advancement and Communications Teams}\label{section.data.advancement}
The Miller Center 's Advancement team maintains several databases to manage their contacts, track giving and fundraising, and to curate our various email lists.  However, these official fundraising databases are owned by University Advancement and are not directly available for use in any third-party AI applications.  In this section, then, we focus on the Advancement-related (and Communications-related) resources that the Miller Center retains direct control over.

A joint effort by Advancement and Communications stores data about every public event that the Miller Center hosts.  In a variety of systems (\href{https://millercenter.org}{millercenter.org}, \href{eventbrite.com}{EventBrite}, and Marketing Cloud), these teams store, for each event:
\begin{itemize}
\item Metadata about the event (title, abstract, participants, location)
\item Registrations
\item Which registrants attended the event (names and demographics).
\end{itemize}
These  are highly structured spreadsheet, XML and JSON data.  

Another crucial source of Miller Center data is our flagship website, \href{https://millercenter.org}{millercenter.org} (mc.org).  We broke out portions of mc.org in previous sections (PRP and POHP).  But Table \ref{table.data.mcorg} re-lists these earlier numbers in order to give a full accounting of the website's holdings in one place.


\begin{table}[htp]
\caption{mc.org data holdings.}
\begin{center}
\begin{tabular}{ p{1.75in} p{2in}}
\toprule
Page Type				&	Number of Pages in mc.org		\\
\midrule
American Forum			&	100		\\
\midrule
General Article Page			&	2,807	\\
\midrule
Public Event				&	3,021	\\
\midrule
Expert					&	71		\\
\midrule
MC Presents				&	150		\\
\midrule
Scroller Page				&	167		\\
\midrule
President Bio				&	45		\\
\midrule
AmPres Bio				&	107		\\
\midrule
Presidential Interview		&	536		\\
\midrule
Presidential Recording		&	33,098	\\
\midrule
Presidential Speech			&	1,050	\\
\bottomrule		
Total						& 	41,152	\\
\bottomrule
\end{tabular}
\end{center}
\label{table.data.mcorg}
\end{table}%



\subsection{Miller Center Data in Amazon Web Services}\label{section.data.aws}
Miller Center IT staff stores the lion’s share of our data (by byte volume) in Amazon Web Services (AWS).  This catch-all descriptor actually contains three subcollections of data.  First, AWS contains the multimedia content that backs \href{https://millercenter.org}{millercenter.org}—items like POHP transcript and briefing book PDFs and PRP audio recordings.   Secondly, we use AWS to house backups of our raw video footage and assets (metadata, images, etc.) related to editing together the final versions of our public event videos.  Lastly, because we use AWS as a backbone of millercenter.org, there is a large body of transaction log data in AWS.  These logs capture how end users use our materials on millercenter.org.  For example, the logs can tell us which presidential speeches, PRP recordings, and POHP interviews are requested most often.

Table \ref{table.data.aws} lists the main holdings in AWS.  

\begin{table}[htp]
\caption{Miller Center Data Holdings: Amazon Web Services (AWS).}
\begin{center}
\begin{tabular}{l l l }
\toprule
Bucket				&	Items		&	\#  Size	\\
\midrule
						\multicolumn{3}{c}{Public}		\\
\midrule
 POHP Briefing Books	&	303			&	0.88 GB \\
 \midrule
 POHP Transcripts		&	2,073		&	2.3 GB	\\
 \midrule
 Presidential Speeches	&	9,439		&	280.5 GB  \\
 \midrule
 PRP Recordings		&	97,124		&	2,300 GB	\\
 \midrule
 FirstYear				&	1,091		&	0.2 GB	\\
 \bottomrule
  Total				&	121,229		&	3,983.82 GB 	\\
 \bottomrule
 						\multicolumn{3}{c}{Backup/Private Data}		\\
 \midrule
 Misc					&	11,199		&	1,400 GB	\\
\midrule
 Transaction Logs		&	1.81M		&	4.3 GB \\
\midrule
 Video Backups			&	448,832		&	620,000 GB \\
\bottomrule
 Total				&	2.26M		&	62004.34 	\\
\bottomrule

\end{tabular}
\end{center}
\label{table.data.aws}
\end{table}%


\subsection{Video Housed in YouTube}\label{section.data.youtube}
The Miller Center has a large collection of digital video, containing recordings of our public events.  An accounting of these assets is available by  surveying our holdings in \href{https://www.youtube.com/@millercenter32}{YouTube}.  We use YouTube to host public copies of our final, edited event programs.  Table \ref{table.data.youtube} describes our YouTube data. 


\begin{table}[htp]
\caption{YouTube video and transcript holdings.}
\begin{center}
\begin{tabular}{p{1.15in} p{.75in} p{.75in} p{.75in} p{.75in} p{.75in}}
\toprule
Source		&	Num. Videos		&	Num. Transcripts	&	Avg. Video Length	&	Avg. Transcript Size	&	Total Size of Transcripts	\\
\bottomrule
Main Channel	&	651			&	627			&	1 hr				&	168k				&	52 MB	\\
\midrule
Am. Forum	&	820			&	776			&	1 hr				&	18k				&	6.5 MB	\\
\bottomrule
Total			&	1,471		&	1,403		&					&					&	58.5 MB	\\
\bottomrule					
\end{tabular}
\end{center}
\label{table.data.youtube}
\end{table}%


\subsection{Presidential Speeches}\label{section.data.speeches}
The Miller Center maintains a corpus of impactful presidential speeches (\href{https://millercenter.org/the-presidency/presidential-speeches}{https://millercenter.org/the-presidency/presidential-speeches}).  This curated collection contains about 1,050 speeches, with entries from all 45 U. S. presidents.  These speeches were enumerated as part of Table \ref{table.data.mcorg}, but we describe them in detail in this section due to their vital importance for AI work.  Each speech in the collection contains:
\begin{itemize}
\item The speech itself (i.e. a transcript)
\item Metadata about the speech (President, date, title, location)
\item An audio recording of the speech (for recent speeches)
\item A video recording of the speech (for recent speeches)
\end{itemize}
Miller Center staff make the editorial decision whether to include each new speech that the president makes.  And occasionally staff elect to include an especially important speech from the past.  So over time this collection grows in size.

While this collection is small, it punches above its weight because it is often requested by data science and political science scholars as a data set to inform their research.  The presidential speech collection is available  via our bulk data acquisition interface, \href{data.millercenter.org}{data.millercenter.org}.  Our data API serves Miller Center data in machine-readable formats.  Maintaining this service is beneficial in two ways:
\begin{enumerate}
\item It allows researchers to download large volumes of our data without increasing the load on our website servers.
\item It exposes our data to researchers in JSON format—a file encoding that is standard in the data science world and which saves consumers the need to remove formatting and encoding information.
\end{enumerate}
We unveiled \href{http://data.millercenter.org}{data.millercenter.org} in 2022 and since then it has become quite popular, with, on average, over 50 downloads per day of the speech collection.


\subsection{Closing Thoughts on Miller Center Data}\label{section.data.closing}
Figure \ref{figure.data_venn} schematizes the various data sources that would be available if the Miller Center undertook a language modeling effort of its own.  The left portion of the figure shows six corpora that we described in this section.  The overlaps between the portions show real-world intersections.  For instance, the PRP transcripts are in part included in the contents of mc.org.  Likewise with the Oral History transcripts: oral history transcript text appears as content on mc.org.  On the other hand, the OHP briefing books, while linked to from mc.org, are themselves completely separate from the website.

Figure \ref{figure.data_venn} shows that we have approximately 43,000 ``pages" of Miller Center-specific text\footnote{We put quotes around \emph{pages} because some content types, such as PRP recording transcripts, may be longer than a single page per item.}.  This is a sufficient amount of text for tasks like model fine-tuning and evaluation.  It would also make an excellent RAG search index.  But for more ambitious modeling tasks we may need to flesh out our local holdings with additional data sources, since deep learning models require such vast training corpora.  Luckily, the research community has made several large English language data sets available.  Three of these are shown in the right half of Figure \ref{figure.data_venn}.  The topmost block in that figure refers to the familiar English language \href{en.wikipedia.org}{wikipedia} database.  ``The Pile" refers to an 800-GB corpus that was used to train several LLMS such as LLama (Facebook) and Palm (Google) \cite{pile:2020}.  The C4 entry refers to the ``Colossal Clean Crawled Corpus" \cite{c4:2021}, another open dataset intended for LLM training.  These resources have all been created, at least in part, in efforts to allow organizations like the Miller Center to supplement their local data holdings in order to build corpora of sufficient size to train LLMs.  

When we discuss data requirements in the following sections, we will refer often to Figure \ref{figure.data_venn} as we imagine different model training and evaluation regimes.




\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.7]{./figures/data_venn.png}
\caption{Available data sources for training language models. N.B. Items are not drawn to scale.}
\label{figure.data_venn}
\end{center}
\end{figure}


\section{AI Initiatives Elsewhere on Grounds}\label{section.grounds}
Because AI work tends to be resource-intensive, finding productive points of collaboration and cross-pollination is an important step as we endeavor to formulate our plans and ambitions in this space.  Not surprisingly, UVA has numerous AI initiatives in the works and recently concluded.  This section outlines the most relevant of these initiatives to this white paper, with the goal of identifying possible avenues for future collaboration and increasing our knowledge of expertise available on grounds.

Many units across grounds have begun AI initiatives.  For our purposes, four of these are the most relevant, due to their high impact and their proximity to the Miller Center, institutionally speaking:
\begin{enumerate}
\item The UVA Futures Initiative: a year-long project that will result in a strategic recommendation to Jim Ryan about AI and the future of universities.
\item The Provost's Generative AI in Teaching and Learning Task Force: an in-depth study of how UVA should integrate AI into teaching.
\item The Vice-Provost for Research's Task Force on AI in Research: a forthcoming survey of how AI is being used in research across grounds.
\item Darden's Artificial Intelligence Initiative (DAII): a website and a roster of public events that crystalize how AI is impacting the business world and the research areas of Darden faculty. 
\end{enumerate}

\subsection{The University Futures Initiative}
Currently, the most ambitious entity in the AI space at UVA is the provost’s Futures Initiative.  The chief aim of the Futures Initiative is to develop a proposal for university leadership that will help UVA work proactively towards a future ushered in by disruptive technologies such as AI and large language models.  A core part of the Futures Initiative is the Futures Initiative Group, a body of internal and external UVA thought leaders that is tasked with strategic thinking about how the work of universities is likely to change in the near future.  

During spring 2024 the aim of the Futures Initiative Group is to form a Future Initiatives Working Group (FIWG), consisting of thought leaders from across Grounds.  The membership of this group will be seeded by recommendations from deans and center directors, and the goal of the group is to pursue focused questions at a subcommittee level.  The formation of the FIWG is an opportunity for the Miller Center to join the larger discussion about AI by nominating personnel for a subcommittee.  This would have the twin benefits of positioning us at an important table and giving us a way to seek points of cross-grounds collaboration in our AI work.

A particularly exciting aspect of the Futures Initiative is its inherent interdisciplinarity and its intentional inclusion of partners from the liberal arts.  During an announcement about the Futures Initiative, Dean of the University’s College and Graduate School of Arts and Sciences, Christa Acampora said, “To understand, adapt, and solve these urgent problems, UVA will need to realize even greater interdisciplinarity across academic fields….The liberal arts and sciences are crucial for this project because—now more than ever—we need a more capacious understanding of what is human, more than human, and humane.”  Though AI is first and foremost a technical field, the generous funding of the Futures Initiative (\$1.5M from the Strategic Investment Fund, for the period 2024-2028) shows that at UVA there will be ample room for humanistic players such as the Miller Center to contribute to the discussion of how best to respond to the challenges posed by novel AI technologies.


\subsection{Provost Task Forces on Artificial Intelligence}
Some of the keenest questions about AI’s role at the university center on teaching and learning—how will AI change the experience of students and instructors?  In spring of 2023 the UVA provost’s office convened a \href{https://provost.virginia.edu/subsite/genai}{Generative AI in Teaching and Learning Task Force}.  The final report of this committee examines how generative AI will affect teaching and learning in three main ways:
\begin{enumerate}
\item What students learn,
\item How students learn, and
\item How learning is assessed.
\end{enumerate}
The outcome of the Generative AI in Teaching and Learning Task Force is twofold.  First, the task force created a \href{https://provost.virginia.edu/subsite/genai/faqs}{web-based tool} intended to help instructors and students to answer practical questions about appropriate uses of AI in UVA classroom settings.  Second, the task force released a \href{https://provost.virginia.edu/subsite/genai/task-force-report}{final report}, outlining its process and findings.  Characterizing fall 2023 as an inflection point, the task force encourages UVA not to “waste a good crisis,” urging faculty to engage in structured thinking about their own plans and university leadership to supply “significant…administrative support” for the UVA community’s efforts to integrate AI into teaching and learning.

With the teaching and learning task force completed, the Vice Provost for Research recently convened a new task force to examine how AI is affecting research efforts across grounds.  This initiative is in its early stages.  But a major item on the task force's agenda is completing a survey of AI-enabled research projects at UVA.  Recently, Raf Alvarado--a member of the AI Research Task Force--interviewed Marc Selverstone and Miles Efron as part of this survey.  He was enthusiastic about the projects we described, and he plans to feature the Miller Center's work in the final report of the task force.  Alvarado also described several funding opportunities, which we return to in Section \ref{section.grounds.funding}

\subsection{The Darden Artificial Intelligence Initiative}
Darden and the Batten Institute have joined forces to form \href{https://www.darden.virginia.edu/intelligence}{UVA Darden’s Artificial Intelligence Initiative} (DAII).  Whereas the Futures Initiative Group and the provost's task forces are intentionally high-level, the DAII is more narrowly scoped, focusing on three main topics:
\begin{itemize}
\item AI and Marketing
\item AI and the Future of Work
\item AI and Economic Progress.
\end{itemize}
To foster critical thinking about these issues, the DAII aims to “[bring] together a diverse, multidisciplinary group of leading scholars…to shape the discussion [on Grounds] about artificial intelligence and its related technologies, embracing a holistic perspective.”  Darden’s AI initiative maintains a program of public events related to these three topics, which will be important to stay abreast of for the Miller Center’s own strategic thinking.


\subsection{Closing Remarks on Cross-Grounds AI Initiatives}
Together, these four initiatives (the Futures Initiative, the two provost-initiated programs, and the Darden AI Initiative) show three important facts about the state of play with respect to AI on Grounds in early 2024:
\begin{enumerate}
\item University leadership sees AI and data-centered computing as a strategic opening that merits investment in terms of strategic planning, teaching, and research.
\item Vital work in this space is being conducted both in centralized and de-centralized initiatives across grounds.  The Miller Center may want to target initiatives and sub-initiatives (e.g. the FIWG) when formulating its own strategic response to these developments.
\item The emphatic inclusion of liberal arts stakeholders in the university’s AI planning presents an opening for the Miller Center to claim a seat at the table, as a uniquely positioned unit, at the nexus of humanistic inquiry and cutting-edge technical work.
\end{enumerate}


\subsection{Resource Availability and the Funding Landscape for AI Work, On- and Off-Grounds}\label{section.grounds.funding}
During our interview with Raf Alvarado for the VPR's AI Research Task Force, we learned that at this time, funding for AI research on Grounds in fairly unstructured but nevertheless available.  The main source of support available for research into AI applications is \href{https://www.rc.virginia.edu}{UVA Research Computing} (URC).  URC offers compute, data storage, and training resources to the UVA community, including special support for AI in the form of GPU access (i.e. the hardware used for AI model training), parallel implementations of the major AI software libraries (e.g. Keras/TensorFlow/PyTorch), and tera-scale data storage.  These resources are available on a pay-as-you-go model.  Raf Alvarado suggested that in the future, UVA will likely offer financial support to help units pay for these services, but at the time, no formal AI support source is broadly available.

As its name implies, the URC computing infrastructure is available only for research purposes.  For any production systems, the Miller Center would need to find other platforms to build on.  However, AI computing resources are increasingly easy to marshal.  The Miller Center has already moved almost all of its IT portfolio to the AWS cloud, and Amazon provides ready access to all of the storage and compute resources that we would need for any AI project.  For specialized services, the major AI platforms are all available via public APIs, which would allow the Miller Center to call, say, GPT-4 models, or the OpenAI embeddings interface from our own code.  Architecting our AI software systems will be a crucial task as we roll out intelligent systems.  But given the increasing industry homogeneity in terms of API exposure, access to models and software is unlikely to pose a significant problem.

To fund our use of these infrastructural elements, a number of new funding opportunities may be of interest.  

The National Science Foundation is currently starting \href{https://nairrpilot.org}{the National AI Research Resource Pilot} (NAIRR),  a project aimed at supporting AI research by providing industry-scale computing infrastructure.  NAIRR's first round of funding--an open call for requests for GPU access--is currently open, with a request deadline of March 1, 2024.  If this date is too early for the Miller Center to submit a request, NSF has indicated that future, possibly more broadly scoped rounds will come online in the future.  In addition to grants for GPU access, NAIRR offers \href{https://nairrpilot.org/pilot-resources}{a web-based repository of AI resources} like pre-trained models, corpora, and software.  

A second possible source of support is \href{https://www.neh.gov/AI}{the National Endowment for the Humanities' Perspectives on AI}, a research initiative that NEH launched in 2023. The Humanities' Perspectives on AI initiative currently has five lines of funding, each with its own focus and target audience.  

The infrastructure and support options available for any Miller Center AI initiatives is certainly broader than what we can outline in this white paper.  However, as this brief section shows, at both local and national levels, there is increasing interest in funding humanistic applications of AI.  Between the Miller Center's current base of support, funding and technology programs on grounds, and commercial and government resources, the resources available to the Miller Center are deep.
 


\section{Possible Applications of AI Technology to Advance the Miller Center's Mission}\label{section.applications}
This section outlines several projects that the Miller Center could undertake to increase its impact and pursue its mission by applying AI technologies.  These proposals are arranged in increasing order of ambition and reach.  

\subsection{A Note on the Cost of AI Projects}\label{section.applications.cost}
The projects we outline in this section would incur several  types of cost.  Some of the computation would take place on servers in the Miller Center's private AWS cloud (about \$10/month per server).  However, the bulk of the proposed work would take the form of calls to public API's (e.g. from OpenAI, AWS, Google).  API costs come in two varieties:
\begin{enumerate}
\item One-time upfront costs for model training
\item Ongoing costs for workload requests.
\end{enumerate}
At this time, AI companies charge for API usage on a per-token basis.  That is, the more words (or audio content) we ask the model to read or create, the higher our cost.  These costs vary from service to service.  

As a point of reference, we offer two cost scenarios, both imagined as a system hosted on OpenAI's cloud.

If we ran an LLM-based service using GPT-3.5 (no fine tuning) that was only for in-house use by scholars, the usage would be fairly low.  If each session involved 100,000 tokens inbound (say, asking the system to read an OHP interview plus instructions) and 2,000 tokens outbound, and scholars initiated 10 sessions per day, the cost  would be about \$24 per month, as shown in Table \ref{table.cost.one}.  The cost of this system would be significantly higher if we needed to rely on a fine-tuned model, as we can see in Table \ref{table.cost.two}.

\begin{table}[htp]
\caption{Costs for a GPT-3.5 system for in-house use only.}
\begin{center}
\begin{tabular}{p{1.55in} p{4in}}
\toprule
Item			&		Cost	\\
\midrule
Input			&	100,000 tokens $\times$ \$0.0005 per 1k tokens $=$ \$0.05	\\
\midrule
Output		&	2,000 tokens $\times$ \$0.0015 per 1k tokens $=$ \$0.003	\\
\midrule
Num. Sessions	&	10	\\
\bottomrule			
Total			&	(\$0.05 $+$ \$0.003)	$\times 10 =$ \$0.53 / day $=$ \$15.9/month	\\
\bottomrule
\end{tabular}
\end{center}
\label{table.cost.one}
\end{table}%

\begin{table}[htp]
\caption{Costs for a fine-tuned GPT-3.5 system for in-house use only.}
\begin{center}
\begin{tabular}{p{1.55in} p{4in}}
\toprule
Item			&		Cost	\\
\midrule
Input			&	100,000 tokens $\times$ \$0.003 per 1k tokens $=$ \$0.3	\\
\midrule
Output		&	2,000 tokens $\times$ \$0.012 per 1k tokens $=$ \$0.024	\\
\midrule
Num. Sessions	&	10	\\
\bottomrule			
Total			&	(\$0.3 $+$ \$0.024)	$\times 10 =$ \$3.24 / day $=$ \$97.2/month	\\
\bottomrule
\end{tabular}
\end{center}
\label{table.cost.two}
\end{table}%

By comparing Tables \ref{table.cost.one} and \ref{table.cost.two}, it is clear that fine tuning raises costs substantially\footnote{The one-time cost of the fine tuning itself is also a factor, though for Miller Center purposes, fine tuning on OpenAI servers is inexpensive.  For example, the cost of fine tuning GPT-3.5 with 400 OHP interviews would be about \$30.00.}.  The other principal cost driver is traffic volume.  In general, costs scale linearly with the amount of traffic sent to the model.  

If we determine that we would like to host a public-facing model on mc.org, the costs for using hosted models would become prohibitive.  In this case, self-hosting our own model  might become an attractive option.  As a point of comparison, hosting a ``small" LLM such as Facebook's Llama-2 7B on AWS would cost approximately \$1,000 per month.  Finding the threshold where a local model becomes cost effective is difficult in general, but a  common industry rule of thumb is that if our traffic generates more than about 1,000 model invocations per day, hosting our model becomes cost effective \cite{vivek:2023, javaness:2023}.  With traffic below about 1,000 calls per day, relying on public APIs is usually cheaper.  

One near certainty is that over time the costs associated with AI will drop.  AI hardware is always declining in cost, and the AI services market is profoundly competitive.  Both of these factors will put downward pressure on AI costs in the coming years.  In the meantime, a common strategy for AI deployment is to start with a system that is highly performant but deployed to a limited audience.  



\subsection{Low-Hanging Fruit: Simple Options with High Impact}\label{section.applications.easy}
Before we introduce more ambitious ideas in Section \ref{section.applications.hard}, we offer several AI applications the Miller Center could undertake that would have high impact but little risk exposure and modest work requirements.  

\subsubsection{Informally Integrating AI Tools into Miller Center Work}\label{section.application.easy.daily-work} 
A key aspect of modern AI tools is their broad application.  Knowing when and how to use AI, and being able to wield AI tools creatively are key skills for in modern office work.  To help the Miller Center (ideally scholars and staff) capitalize on the current state of the art, a few simple actions could go a long way.

First, it could be advantageous to schedule a periodic brown bag or informal tutorial on creative applications of AI.  We could draw on local talent, and optionally invite external guests to participate in such a forum.  One goal might be for a designated staff member (e.g. Miles Efron) to handle at least the first installments of such a brown bag series, with the goal of providing instruction in using these tools for the kinds of tasks that come up often at the Miller Center.  
 Initial topics could include prompt engineering best practices, using ChatGPT to analyze spreadsheet data, and a comparison of AI tools such as ChatGPT, Bard and Anthropic's Claude.  After time, once it has momentum, the series could be opened to guest speakers who could present on their own experiences using AI in their work.

Concomitant with a live instructional series, we could make a living document about AI technologies available to Miller Center staff.  The document could contain, among other things:
\begin{itemize}
\item Reviews of online courses on AI.  Currently, UVA makes numerous trainings available to staff, and highlighting the best of these might be useful.
\item Reports on successful use of AI in the workplace.
\item A listing of tools available to Miller Center staff and descriptions of their applications to our work.
\item Guidelines for effective prompt engineering.
\end{itemize}
The goal of this document would be to help Miller Center staff integrate AI into their daily workflow by highlighting tasks and settings where AI can offer real benefits for completing daily tasks.







\subsubsection{Expanding our Data Portal, data.millercenter.org}\label{section.applications.easy.data-portal}
Miller Center staff rolled out a data portal, \href{https://data.millercenter.org}{data.millercenter.org}, and its API backend \href{https://api.millercenter.org}{api.millercenter.org}, in 2022\footnote{For clarification \href{https://data.millercenter.org}{data.millercenter.org} is a human-readable website that explains our data offerings and contains documentation for our data API.  On the other hand, \ \href{https://api.millercenter.org}{api.millercenter.org} is the endpoint of our data API.  It has no human-readable content and is only intended for interaction with AI software using the REST protocol.}.  The goal of these services is to allow researchers to download Miller Center content in research-friendly formats without overloading our production web servers.  Since unveiling these services, we have seen consistent interest in them, with about 50 data downloads per day.  Currently, the data portal only serves the text and  metadata associated with the 1,050 speeches in the millercenter.org presidential speech collection.   A logical and easy improvement we could make would be to flesh out our data offerings, increasing the types of data we serve and the functionality of our data API.  These changes would help increase the Miller Center's profile in the data and natural language processing (NLP) communities.

To increase the value of our data API, there are two ways we might expand our data offerings.  

First, we can add additional types of data to our API.  In addition to the presidential speeches, it would be a simple matter to make available the POHP transcripts that we already host on  \href{https://millercenter.org}{millercenter.org}, assuming copyright issues would permit it.  Likewise, for each of the “scrollers” that we host on the Listening to the Presidency website section, we could add the associated transcript to the data portal.   Again, copyright issues would dictate the ultimate decision to include or exclude transcripts. 

Second, we could add richer metadata and functionality to the data portal’s underlying REST API.  For example, an important step in most AI work with natural language text is deriving so-called “text embeddings.”  Text embeddings are themselves an AI product.  They represent natural language text in numerical form, such that the text’s underlying semantics become computationally tractable.  The first step of almost all modern AI work involves transforming input text into embeddings.  We could increase the utility of  \href{https://datamillercenter.org}{data.millercenter.org} by pre-computing text embeddings for all exposed text and then making those embeddings available to clients.  This would greatly enhance the value of our service to downstream researchers.  

The aim of improving our data service would be, in part, to strengthen public perceptions of the Miller Center as a force in the NLP and data community.  As part of this effort, we might announce API improvements on standard NLP and AI lists.  For researchers and students (our data is often used for AI course projects) \href{https://data.millercenter.org}{data.millercenter.org} is a useful tool.  Investing in increasing its functionality, offerings, and visibility could be a boon to the Miller Center's reputation in the data and AI communities.



\subsubsection{Improving the \href{https://millercenter.org}{mc.org} Site Search Engine}\label{section.applications.easy.sitesearch}
In 2023 the Miller Center Web team overhauled the search engine that supports our flagship website, \href{https://millercenter.org}{mc.org}.  Search plays an invaluable part of users' experience with our website--visitors used our search engine almost 200,000 times in 2023, and since unveiling our search improvements, search volume has doubled.  Given the core role our search engine plays in users' interaction with mc.org, it would be advantageous to use AI to improve our search engine.

Due to technical limitations, there are currently two major gaps in our site search engine's index:
\begin{enumerate}
\item Presidential recordings.  People can search our corpus of recordings by metadata (title, speaker name, date), but not the content of each recording.  
\item Public events.  As with the recordings, people can find information about events based on metadata, but not based on the spoken contents of each public event.
\end{enumerate}
AI presents a clear path to filling these gaps.  To improve our site search engine, we could add invisible text fields to our web content.  The new content would be obtained via AI.  It would never be displayed to humans.  But we could use it to index the pages, allowing users to search the recordings and events by content.

In terms of the presidential recordings, current-generation audio-to-text models are sufficiently accurate to transcribe the recordings \emph{for search purposes}.  That is, we would only use these transcriptions to support search, not as a visible field when displaying content on mc.org.  It may be the case that the noisiest of the recordings would remain un-transcribable by AI.  However, measuring the ``noisiness" of these recordings is an  task that we could automate.  Based on experimentation, we could easily derive a threshold of clarity, only using AI to transcribe those recordings whose audio quality exceeds a certain value.

For the public events, no additional processing is needed.  Since we host videos of our events on YouTube, Google creates transcripts of our events automatically.  To add these transcriptions to our search engine, all we need to do is to download them and attach each transcript to its corresponding event on the site.  

The main work of this modest project would be $A$) assessing how to handle the problem of noisy recordings and then $B$) making the needed changes to the website.  The work of attaching the transcripts to their pages on mc.org would be a fairly simple programming task.  Finally, we would need to build some user interface changes into the site search engine to allow users to include or exclude the AI-generated full-text search.  However, none of these tasks is difficult.

In terms of cost, the main cost would come from obtaining the AI-generated recording transcripts.  However, even this is a modest outlay.  If we transcribed every minute of audio shown in Table \ref{table.data.prp} using OpenAI's \href{https://openai.com/research/whisper}{Whisper} audio-to-text AI, the cost would be about \$1,725.  If we limited transcription to relatively ``clean" recordings, this cost would go down.


\subsection{Ambitious, Moonshot-Type Applications}\label{section.applications.hard}
While our previous section offered simple, practical ways to leverage AI technologies at the Miller Center, in this section, we offer more ambitious recommendations.  How could the Miller Center marshal AI’s boldest promises to revolutionize its work and qualitatively extend its impact?  This section recommends three approaches.


\subsubsection{A Miller Center Language Model}\label{section.applications.hard.mcllm}
During discussions about future AI initiatives at the Miller Center, several scholars have imagined the value of a native Miller Center large language model.  The aim of a Miller Center LLM would be to support wide-ranging, interactive queries into presidential history and politics, drawing on the Miller Center's unique data to formulate answers.

At least three concrete use cases for a Miller Center LLM arose during preparation of this white paper:
\begin{itemize}
\item Miller Center CEO Bill Antholis imagined a system where users could ask to see all conversations where presidents discuss U.S.-China relations. 
\item Miller Center scholar Guian McKee discussed the research value of a system that could identify all moments in PRP recordings where presidents or their staff spoke in ways that violate the public trust.  
\item Miller Center scholar Barbara Perry suggested that an AI-based system could help researchers conduct a follow-up to the \href{firstyear2017.org}{FirstYear project}, focusing on presidential second terms.
\end{itemize}
Each of these cases speaks to the same underlying challenge: for researchers and other Miller Center audiences, there is a tremendous opportunity in finding a way to bring novel context to  the entirety of the Miller Center's data universe.  Building a system that would allow its users to interact with Miller Center data semantically--seeking themes and trends, not simply keywords and tags--is arguably \emph{the} breakthrough that AI affords us.

To an end user, such a system might look like a ChatGPT-style conversational agent.  The goal would be to allow the user to enter an initial query and then re-query the model based on the first results.  Through an iterative process, users should be able to retrieve data, request  summaries of the retrieved data, and add follow-on queries based on these summaries.  Each of the use-cases listed above is similar insofar as the questions seek a retrieved data set that is in some sense finite, but whose edges are not clear cut (not all discussions of U.S.-China relations, breeched public trust, or presidential second terms are clearly labeled as such).  This semantic similarity and the underlying interest in iterative, incremental querying that would distinguish a Miller Center LLM from a more familiar search application.

There are numerous architectures developers could plausibly use to build this ``Miller Center LLM."  A full reckoning of this decision is outside our current scope.  But for the remainder of this section, we offer initial thoughts into this project's design decisions.  Our goal here is not to finalize the system, but to help readers understand what decisions would need to be made, and what resources secured, to bring this to completion.

In light of our discussion of the Miller Center's data holdings in Section \ref{section.data}, there are three main architectures a system like this could rely on.

A first approach would be to build, from scratch, a true Miller Center LLM.  Doing this would necessitate using all of the data sources (internal and external) shown in Figure \ref{figure.data_venn}.  In order to give our model enough training data, we would need to supplement our data crown jewels with external sources such as Wikipedia, the ``pile" and the C4 corpus.  This would be a hefty engineering task.  It would require us to find adequate storage and GPU/compute resources.  However, this is a plausible path forward.

A more modest approach would be to use our internal data sources (i.e. the left half of Figure \ref{figure.data_venn}) to fine-tune an existing foundation LLM.  This approach would have several advantages over a from-scratch LLM.  For instance, it would require much lighter resource allocation, both in storage and compute.  Secondly, it would free development from the emphatically non-trivial data cleaning and data selection work that training a full LLM would necessarily require.  However, fine tuning is most successful when we want the resulting model to perform well on single, well-defined type of task.  In this case, initial discussions sound like retaining flexibility in our final model is important, a consideration that may argue against the fine tuning approach.

Perhaps the clearest path  to developing a system like this is not to train a new LLM at all, but rather to create a retrieval-augmented generation (RAG) system.  As we discussed in Section \ref{section.definitions.rag}, RAG systems combine a search engine and an LLM to create a conversational system that ``knows" information stored in text documents.  The advantage of RAG for a problem like this one is that it will allow us to capitalize most directly on our data.  With a purely LLM-driven system, the knowledge in our data would only be retrievable indirectly, insofar as it ends up encoded in the model's parameters.  In a RAG system, however, the model would essentially act as a broker, finding passages from our documents that using a general-purpose LLM to contextualize what the passages say.


\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.7]{./figures/mc_llm.png}
\caption{Architecture of a Miller Center LLM/RAG question answering system.}
\label{figure.mcllm}
\end{center}
\end{figure}

Figure \ref{figure.mcllm} sketches a likely architecture for a Miller Center LLM/RAG system.  Information would enter the system via this pipeline:
\begin{enumerate}
\item Our documents--recordings, interviews, public event transcripts, and mc.org content--would be translated in ``text embeddings" by a service such as OpenAI's.  These text embeddings translate each document (or sub-document chunk) into a representation amenable to use by LLMs.
\item A vector database stores the embeddings.  This database is essentially the index of our RAG system.  Pinecone is a cloud-based vector database provider that would be a likely candidate for this task.
\item Miller Center staff would develop software using the OpenAI ChatGPT and LangChain API's.  This software handles incoming user queries, searches over the vector database, and finally, translates retrieved documents into conversational English.
\end{enumerate}
Users could interact with the system via a new portal on mc.org, or on a newly created website; the point is that a familiar web interface would host the service.  Incoming queries would interact with the OpenAI/LangChain code (hosted in AWS, probably), and the final results and interaction would be handled by the web interface.

This sketch is a fully plausible way to build a system that could support broad queries against Miller Center information.  Given the novelty and ambitiousness of this task, however, it would be important to consider in-depth planning and research before settling on a final system architecture.  

An important design question related to a Miller Center ``LLM" is who the supported audience will be.  If the tool will be limited to use by Miller Center scholars, we could deploy it with fairly modest costs.  The services shown in Figure \ref{figure.mcllm} all charge based on usage volume.  A system along the lines of Figure \ref{figure.mcllm} could be created with minimal upfront costs (on the order of \$1,000-\$5,000, though further research is needed to derive a concrete estimate).  Serving this application for internal use only would require very low ongoing costs after the initial outlay.  However, if our intention is to serve this model to the full mc.org audience, costs will rise substantially.   




\subsubsection{A Digital Forensics Kit to Support Presidential Recordings Scholarship}\label{section.applications.hard.prp}
Current-generation audio-to-text AI is sufficiently powerful to handle tasks like transcribing Miller Center public events and extracting text from PRP recordings to support search engine indexing.  But even modern AI has important limitations when it comes to the research problems faced by PRP scholars.  First, current models cannot identify who is speaking during an audio recording.  Secondly, audio-to-text technology cannot accurately transcribe passages where aging media or recording artifacts render the audio itself noisy.   Because of these limitations, AI applications for PRP will have to look beyond off-the-shelf offerings.  


Instead of using AI to replace the transcriptive work of scholars, a more feasible approach is to use AI to give scholars new tools that might help them work more efficiently or effectively.  In this section we propose two innovations that PRP scholars could use to arrive at forensic insights into the contents audio recordings.

To help scholars identify speakers, a plausible path forward is to fine tune an audio classification model.  Models of this kind take an audio clip as input and return text as their output.  In this case we could begin with a foundation model trained to understand recordings of English language conversations.  To align this model with PRP needs, we would fine tune it by presenting it with a training collection of labeled audio clips as in Figure \ref{figure.prpspeakers}.  The left portion of Figure \ref{figure.prpspeakers} schematizes the training corpus.  This would consist of \{\emph{audio clip, speaker name}\} pairs.  This training data would allow the model to learn what JFK's voice (or LBJ's voice, or J. Edgar Hoover's voice) sounds like,.  To use the fine-tuned model, PRP scholars would indicate that they would like an assessment of who is speaking at a particular time code in a particular audio file.  The model would return a ranked list of possible speakers for the clip in question, along with the probability that each speaker is the correct label.  

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=1.0]{./figures/prp_speakers.png}
\caption{Training and Query Regime for a Speaker Identification Tool.}
\label{figure.prpspeakers}
\end{center}
\end{figure}

A similar tool could help PRP scholars with another common hurdle in their work, transcribing passages that are inaudible.  The original White House recordings are often noisy, and in many cases it is difficult  to make out how a conversation goes.  To help scholars fill in these blanks, a combination of LLMs and audio models could be brought to bear.  The idea is that scholars would present to a model the parts of the conversation they \emph{do} know, and an indication of the part they cannot make out.  Based on the given context, the model would return a ranked list of probable transcriptions for the inaudible portion.  For instance, if scholars had transcribed a sentence as \emph{Apparently some \{unclear\} in Justice is lobbying with the Post}, an LLM could induce a probability distribution over all words $X$ in the English language $P(X | $ Apparently some \{unclear\}, \{unclear\} in justice is lobbying with the post $)$.

The training data for both of these tasks is readily available.  For the speaker identification task, training data could come from two sources:
\begin{itemize}
\item PRP maintains a voice ID bank, a database of sound files, each of which is labeled with the name of the person who is heard speaking in the audio.
\item ``scroller" presentations on mc.org: each of these multimedia documents contains an audio file and time-coded lists of who is speaking at each moment of the audio.
\end{itemize}
Together these two data sources could train a model to make a judgment of who is speaking in a new, un-labeled recording.

For a system that suggests transcriptions of noisy audio passages, an off-the-shelf foundation LLM might suffice.  A fine-tuned model might improve over the foundation model.  But the core point in this case is that this task--filling in gaps in English language text--is precisely how LLMs operate.  An LLM is trained by showing it terabytes of text.  At each step in the training process, random words in the training text are ``masked" (hidden).  The model is trained by teaching it to guess correctly which words should go into those masked slots.  This is precisely the task we would ask the model to do in the PRP case, though here the gaps may contain longer passages than a single word.  Nevertheless, the setup is identical.

It is important to stress that these two applications for PRP would constitute novel research in their own right.  It seems plausible and likely that these systems could be trained to deliver predictions that would give PRP scholars a useful new perspective on their work.  However, to bring these models up to a useful level of accuracy will require non-trivial scientific (not just engineering) thinking.  This would be an exciting, impactful project that would be of scientific interest to the AI community, in addition to its value as a tool to advance PRP scholarship.


\subsubsection{Fine-Tuned LLMs to Create Oral History Interview Metadata}\label{section.applications.hard.pohp}
The Miller Center's Presidential Oral History Project (POHP) generates rich interviews with former White House personnel and other presidential confidants.  To maximize the value of the corpus of these interviews, two summary bits of metadata are desirable:
\begin{enumerate}
\item \emph{Page-wise indexing}: POHP staff have long desired a full index of all published OHP interviews to support internal organization and retrieval operations.
\item \emph{Interview-wise summarization}: When interviews are published, they require an accompanying pr\'{e}cis, a brief prose description of the interview contents.
\end{enumerate} 
Both of these operations--indexing interviews and summarizing them into a pr\'{e}cis--are laborious tasks for humans, but plausibly something an LLM could automate.\footnote{The same indexing operations that we are outlining could be useful for PRP data too, if there is an interest in that approach.  The indexing methodology described in this section could be applied to recording transcripts exactly as we have laid out here.}

Again, the exact design specification for this task's solution is a complex question.  But a likely architecture  would use LLM fine-tuning.   The most direct path towards automating the creation of both OHP metadata types (indexing and pr\'{e}cis), would be to choose a foundation model such as GPT-4 and then to fine tune it twice, once for each task, page indexing and interview pr\'{e}cis creation.  Training two separate models is desirable in this case because we are training for two distinct tasks.  Separating the fine tuning process would prevent the model from confusing the two tasks, possibly diminishing the needed volume of training data.  

To train an OHP interview indexing model, the training data would consist of \{interview page, indexing metadata\} pairs.  That is, for each training page, humans would enumerate which indexing terms they would assign to that page.  Feeding 50-100 indexed pages to the foundation model should give it enough information to generalize well on unseen pages (the real number needed may be substantially lower than 50, as initial experiments suggest that even GPT-4 with no fine tuning performs well on this task.)

\begin{figure}[htbp]
\begin{lstlisting}[language=Python]
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
)
def get_completion(prompt):
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": prompt
            }
        ],
        model="gpt-4",
    )
    return chat_completion.choices[0].message.content
def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        num_pages = len(reader.pages)
        for i in range(num_pages):
            page = reader.pages[i]
            yield i, page.extract_text()

def main(pdf_path):
    page_number=0
    for page_number, text in extract_text_from_pdf(pdf_path):
        page_number += 1
        prompt = "List all people, countries , and government agencies contained in the following text: " + text
        completion = get_completion(prompt)
        print(str(page_number), completion)

if __name__ == "__main__":
    pdf_path = "./data/Abrams_Elliot.final2.pdf"
    main(pdf_path)
\end{lstlisting}
\caption{A sample python program for automatically extracting indexing terms from an OHP interview.}
\label{figure.python}
\end{figure}

To show how the indexing process could be fully automated, Figure \ref{figure.python} shows a simple program that generates page-wise indexing terms for an Oral History interview.  This example gets its data from the GPT-4 foundation model but changing this to pull from a fine-tuned model is trivial (simply change line 12 of the program).  By hosting this program in the Miller Center's AWS account we could create a system that automatically indexes any new interviews that we publish on mc.org (or that we otherwise mark as ``complete").

Creating interview pr\'{e}cis documents could be automated in a similar way.  In this case, instead of training our model in a page-wise fashion, our training data would entail a set of \{interview, precis\} pairs.  By using the set of interviews on mc.org that already have a pr\'{e}cis, we can build our training set trivially.  To train our pr\'{e}cis model, we simply upload this training set to the fine-tuning API.  The available set of interview-pr\'{e}cis pairs should more than suffice for this training.

There is, however, one point of difficulty that we must solve before deploying an automatic pr\'{e}cis creation tool.  The problem has to do with the ``context window" of LLMs.  The context window is  the maximum allowable length of an LLM's prompts.  Each model's context window is different, and for the most part, larger models have larger context windows.  GPT-4's context window is 8,000 tokens (approximately 8,000 words).  For comparison, the Elliott Abrams interview shown in Figure \ref{figure.python} is 95,693 tokens, as per OpenAI's counting method.  Thus we must devise a way to extract a single, summative pr\'{e}cis from GPT-4 (or any fine-tuned model derived from it), when we can only show $\frac{1}{11}^{th}$ of the interview to the model at a time\footnote{N.B. When we interact with ChatGPT via a web browser and upload a long PDF for summarization, behind the scenes ChatGPT removes any tokens from the supplied PDF that exceed the context window length.}.

How to overcome the context window limitation is  a  research question in its own right.  However, our initial experiments suggest that an iterative algorithm can be used to solve this issue.  The algorithm runs as follows:
\begin{enumerate}
\item Begin with input interview document $D$.
\item Split $D$ into $n$ sub-documents $D_1, D_2, \ldots, D_n$ such that  for each sub-document $D_i$ $length(D_i < 8000)$.
\item For each sub-document $D_i$ generate a corresponding ``pseudo pr\'{e}cis" $P_i$ by submitting $D_i$ for summarization by the model.  This results in a set of $n$ pseudo-pr\'{e}cis $P_1, P_2, \ldots, P_n$.
\item Generate a final pr\'{e}cis $P$ by submitting the pseudo-pr\'{e}cis to the model with a prompt asking the model to combine them into a single, unified summary.
\end{enumerate}
Testing and troubleshooting this algorithm will require  work, but initial experiments suggest that it is feasible and practical.  By applying this algorithm, we should be able to obtain coherent summaries for OHP interviews.



\subsection{Hazards Associated with AI}\label{section.projects.hazards}
The opportunities that AI affords the Miller Center are easy to see.  The projects we have outlined in this section would bring new efficiencies to Miller Center work, and they are likely to help scholars make novel insights in their research.  Some deployments would greatly enhance the experience of visitors to our flagship website.  But adopting AI also brings  risks for the Miller Center.  These risks fall into three main types:
\begin{enumerate}
\item \emph{Financial:}  Implementing some of the systems described in this white paper will require significant outlays of money.  In particular, deploying an AI model publicly could require steep investment.  The good news on this count, however, is that these costs are predictable; we will always know where the financial pinch points are and can monitor them carefully.  Also, like all technology, the cost of AI infrastructure is sure to drop in the coming years, mitigating some of this risk.
\item \emph{Internal:} Integrating AI into office  and scholarly workflows brings some risk of introducing errors or other quality degradations.  For AI to work effectively, relevant staff must be instructed on best practices and risk mitigation.  Likewise, managers must be circumspect in recommending AI as a solution to work problems.  For instance, a controlled study found that creative office tasks often benefit from AI input, while AI can negatively impact the quality of output on rote, administrative tasks \cite{candelon:2023}.  
\item \emph{Outward:}  If the Miller Center were to make a public-facing AI agent (e.g. deploying its Miller Center LLM on the open internet via mc.org), the results are unpredictable.  RAG systems are prone to fewer ``hallucinations'' than pure LLMs are, but even RAG systems can  give bad answers, and even with guardrails in place, AI systems can be coaxed to generate toxic content.  Deploying an AI agent for the public could be helpful and impactful, but the risks of a public deployment are substantial.
\end{enumerate}
These  are the most obvious risks that the Miller Center would face in the context of formulating an AI strategy.  But in the background, more fundamental concerns about AI are also present.  Ongoing research in the AI community points to developers' concerns about fairness, bias and toxicity in AI \cite{cheatham:2019, hendrycks:2023}.  Because these models rely on vast troves of training data, and because that data must, by virtue of its scale, come from the internet, we know that LLMs have been trained on content that is offensive and factually wrong.  How to prevent models from letting toxic content color their output is a crucial and ongoing area of research.  

AI's hazards are real.  But for the most part they are controllable and avoidable.  So long as decision makers and developers build caution and supervision into their plans, AI's benefits are likely to outweigh its risks.



\section{Conclusion}\label{section.conclusion}
The Miller Center is poised to use AI technologies in ways that deepen its already first-rate scholarship.  Innovations such as LLM-based pr\'{e}cis generation for OHP interviews, forensic audio analysis for PRP transcription, and a Miller Center LLM/RAG system would allow scholars to approach  familiar research activities from a new perspective.  On the other hand, less lofty applications--bringing AI into staff's daily office work and using AI to improve mc.org site search--constitute clear steps towards enhancing the Miller Center's institutional outreach and internal efficiency.  Collectively, these options give the Miller Center a spectrum of tangible ways to initiate a bold technical and intellectual move by adopting AI.

If the Miller Center chooses to move forward with a slate of AI projects, some of the resultant benefits are easy to predict.  At the most transactional level, AI has been shown to improve office productivity \cite{candelon:2023}, and the Miller Center is likely to find tangible benefits from bringing AI into its everyday work.  But more exciting advances are also likely to follow from a strategic adoption of AI.  The Miller Center possesses a unique portfolio of data, and AI would give us an apparatus for using these data in truly novel ways.  Lastly, several of the AI projects  proposed in Section \ref{section.applications.hard} would help advance the state of the art in AI, constituting original research.  As leadership considers a future for AI at the Miller Center, balancing these (and possibly additional, unknown) benefits with the risks and costs described in Section \ref{section.applications} with be of paramount importance.


The goal of this white paper has been to give leadership the tools it needs to strike this balance.  We grounded our discussion by offering a set of crucial definitions and a shared vocabulary in Section \ref{section.definitions}.  Our data census in Section \ref{section.data} outlined the extent, formats, and ownership details of our data holdings.  In Section \ref{section.grounds} we outlined the most vital AI initiatives currently and recently underway at UVA, taking pains to show how the Miller Center fits into the AI landscape on grounds.  Finally, Section \ref{section.applications} details seven ``shovel-ready" AI projects the Miller Center could undertake, each of which balances AI's risks and rewards differently, delivering a unique spectrum of value propositions.  Novel semantic  technologies such as LLMs and multimodal deep learning have made data valuable in a qualitatively new way.  Organizations can use data (and models trained from data) to make insights and efficiencies that  were out of reach only a few years ago.  Given its unique data portfolio, its scholarly position vis-\`{a}-vis that data, and its in-house technical resources, the Miller Center is in a position to use AI in ways that substantially increase the reach and impact of its work. 










\pagebreak
\bibliographystyle{apalike}
\bibliography{mcai}

\end{document}  